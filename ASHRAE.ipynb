{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for ASHRAE on Kaggle \n",
    "https://www.kaggle.com/c/ashrae-energy-prediction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "weather_test = pd.read_csv('data/weather_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('data/building_metadata.csv')\n",
    "weather_train = pd.read_csv('data/weather_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = train.merge(metadata, on = 'building_id',how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train['site_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(weather_train,on = ['timestamp','site_id'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.corr()['meter_reading'].sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['meter_reading'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['floor_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['primary_use'].value_counts() \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage = {'Education':0,'Office':1,'Entertainment/public assembly':2,\n",
    "         'Lodging/residential':3,'Public services':4,'Healthcare':5,\n",
    "         'Other':6,'Parking':7,'Manufacturing/industrial':8,\n",
    "        'Food sales and service':9,'Retail':10,'Warehouse/storage':11,\n",
    "        'Services':12,'Technology/science':13,'Utility':14,\n",
    "        'Religious worship':15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_func = lambda x: usage[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['primary_use'] = merged_df['primary_use'].apply(usage_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(merged_df['square_feet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['year_built'] = merged_df['year_built']-1900\n",
    "merged_df['square_feet'] = np.log(merged_df['square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degToCompass(num):\n",
    "    val=int((num/22.5)+.5)\n",
    "    arr=[i for i in range(0,16)]\n",
    "    return arr[(val % 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "            \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "            \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "            \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "            \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "            \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "            \"2019-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"timestamp\"] = pd.to_datetime(merged_df[\"timestamp\"])\n",
    "merged_df[\"is_holiday\"] = (merged_df['timestamp'].dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "merged_df[\"weekday\"] = merged_df[\"timestamp\"].dt.weekday\n",
    "merged_df[\"hour\"] = merged_df[\"timestamp\"].dt.hour\n",
    "merged_df[\"weekday\"] = merged_df['weekday'].astype(np.uint8)\n",
    "merged_df[\"hour\"] = merged_df['hour'].astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_imputation(df, column_name):\n",
    "    imputation = df.groupby(['timestamp'])[column_name].mean()\n",
    "    \n",
    "    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n",
    "    del imputation\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = average_imputation(merged_df, 'wind_speed')\n",
    "merged_df = average_imputation(merged_df, 'wind_direction')\n",
    "\n",
    "beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n",
    "          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n",
    "\n",
    "for item in beaufort:\n",
    "    merged_df.loc[(merged_df['wind_speed']>=item[1]) & (merged_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['meter_reading_log1p'] = np.log1p(merged_df['meter_reading'])\n",
    "df_group = merged_df.groupby('building_id')['meter_reading_log1p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['wind_direction'] = merged_df['wind_direction'].apply(degToCompass)\n",
    "merged_df['beaufort_scale'] = merged_df['beaufort_scale'].astype(np.uint8)\n",
    "merged_df[\"wind_direction\"] = merged_df['wind_direction'].astype(np.uint8)\n",
    "merged_df[\"meter\"] = merged_df['meter'].astype(np.uint8)\n",
    "merged_df[\"site_id\"] = merged_df['site_id'].astype(np.uint8)\n",
    "\n",
    "building_median = df_group.median().astype(np.float16)\n",
    "merged_df['building_median'] = merged_df['building_id'].map(building_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import (roc_curve, auc, accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_c = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del merged_df_c['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REducing memory\n",
    "test = reduce_mem_usage(test)\n",
    "\n",
    "weather_test = reduce_mem_usage(weather_test)\n",
    "meta = reduce_mem_usage(metadata)\n",
    "\n",
    "merged_df = reduce_mem_usage(merged_df)\n",
    "merged_df_c = reduce_mem_usage(merged_df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(merged_df_c['meter_reading'])#.values\n",
    "del merged_df_c['meter_reading']\n",
    "x = merged_df_c.select_dtypes(exclude = ['object'])#.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = [\"site_id\",\n",
    "                \"building_id\",\"building_median\",\n",
    "                \"primary_use\", \"hour\", \"weekday\", \"meter\",  \"wind_direction\", \"is_holiday\"]\n",
    "\n",
    "drop_cols = [\"sea_level_pressure\", \"wind_speed\"]\n",
    "\n",
    "numericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n",
    "              \"dew_temperature\", 'precip_depth_1_hr', 'floor_count', 'beaufort_scale']\n",
    "\n",
    "feat_cols = categoricals + numericals\n",
    "\n",
    "x = x.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'metric': {'rmse'},\n",
    "            'subsample': 0.25,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.4,\n",
    "            'num_leaves': 42,\n",
    "            'feature_fraction': 0.9,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1\n",
    "            }\n",
    "\n",
    "folds = 4\n",
    "seed = 111\n",
    "\n",
    "kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "models = []\n",
    "for train_index, val_index in kf.split(x, x['building_id']):\n",
    "    train_X = x[feat_cols].iloc[train_index]\n",
    "    val_X = x[feat_cols].iloc[val_index]\n",
    "    train_y = y.iloc[train_index]\n",
    "    val_y = y.iloc[val_index]\n",
    "    lgb_train = lightgbm.Dataset(train_X, train_y, categorical_feature=categoricals)\n",
    "    lgb_eval = lightgbm.Dataset(val_X, val_y, categorical_feature=categoricals)\n",
    "    gbm = lightgbm.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=(lgb_train, lgb_eval),\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval = 100)\n",
    "    models.append(gbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importance(model,columns = feat_cols):\n",
    "    feature_imp = pd.DataFrame(sorted(zip(model.feature_importance(),columns), reverse=True), columns=[\"Value\",\"Feature\"])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    importance_bar = sns.barplot(data=feature_imp, x='Value', y='Feature')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_importance(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_feature_importance(models[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, train_X, val_X, lgb_train, lgb_eval, train_y, val_y, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = test.merge(metadata, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n",
    "\n",
    "test_merged[\"primary_use\"] = test_merged[\"primary_use\"].apply(usage_func)\n",
    "\n",
    "test_merged = test_merged.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_median = df_group.median().astype(np.float16)\n",
    "test_merged['building_median'] = test_merged['building_id'].map(building_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged[\"timestamp\"] = pd.to_datetime(test_merged[\"timestamp\"])\n",
    "test_merged[\"is_holiday\"] = (test_merged['timestamp'].dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "test_merged[\"hour\"] = test_merged[\"timestamp\"].dt.hour\n",
    "test_merged[\"weekday\"] = test_merged[\"timestamp\"].dt.weekday\n",
    "test_merged[\"weekday\"] = test_merged['weekday'].astype(np.uint8)\n",
    "test_merged[\"hour\"] = test_merged['hour'].astype(np.uint8)\n",
    "test_merged['year_built'] = test_merged['year_built']-1900\n",
    "test_merged['square_feet'] = np.log(test_merged['square_feet'])\n",
    "\n",
    "test_merged = average_imputation(test_merged, 'wind_speed')\n",
    "test_merged = average_imputation(test_merged, 'wind_direction')\n",
    "\n",
    "for item in beaufort:\n",
    "    test_merged.loc[(test_merged['wind_speed']>=item[1]) & (test_merged['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n",
    "test_merged['wind_direction'] = test_merged['wind_direction'].apply(degToCompass)\n",
    "\n",
    "test_merged['wind_direction'] = test_merged['wind_direction'].apply(degToCompass)\n",
    "test_merged['beaufort_scale'] = test_merged['beaufort_scale'].astype(np.uint8)\n",
    "test_merged[\"wind_direction\"] = test_merged['wind_direction'].astype(np.uint8)\n",
    "test_merged[\"meter\"] = test_merged['meter'].astype(np.uint8)\n",
    "test_merged[\"site_id\"] = test_merged['site_id'].astype(np.uint8)\n",
    "\n",
    "test_merged = test_merged[feat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "res=[]\n",
    "step_size = 50000\n",
    "for j in tqdm(range(int(np.ceil(test.shape[0]/50000)))):\n",
    "    res.append(np.expm1(sum([model.predict(test_merged.iloc[i:i+step_size]) for model in models])/folds))\n",
    "    i+=step_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.concatenate(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['meter_reading'] = res\n",
    "submission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
